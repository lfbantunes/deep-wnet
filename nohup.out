2019-02-27 02:30:49.719038: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-27 02:30:49.886463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-27 02:30:49.886900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 9.72GiB
2019-02-27 02:30:49.886915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-27 02:30:50.419259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-27 02:30:50.419290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-02-27 02:30:50.419295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-02-27 02:30:50.419832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9395 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-02-27 02:31:02.558257: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.31GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:02.590416: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.67GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:02.639387: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.93GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:02.660834: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.58GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:02.715616: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.45GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:02.733960: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.37GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:02.808540: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:02.838502: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:02.934560: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-02-27 02:31:03.036028: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.15GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Reading images
2_10 read
2_11 read
3_10 read
3_11 read
4_10 read
4_11 read
5_10 read
5_11 read
6_7 read
6_8 read
6_9 read
6_10 read
6_11 read
7_7 read
7_8 read
7_9 read
7_10 read
7_11 read
2_12 read
3_12 read
4_12 read
5_12 read
6_12 read
7_12 read
Images were read
Train with  18  images and validation with  6  images.
start train net
Generated 4000 patches
Generated 1000 patches
Train on 4000 samples, validate on 1000 samples
Epoch 1/150
 - 38s - loss: 0.3950 - val_loss: 0.5450
Epoch 2/150
 - 31s - loss: 0.3790 - val_loss: 0.5544
Epoch 3/150
 - 31s - loss: 0.3673 - val_loss: 0.6095
Epoch 4/150
 - 32s - loss: 0.3651 - val_loss: 0.5931
Epoch 5/150
 - 32s - loss: 0.3615 - val_loss: 0.5659
Epoch 6/150
 - 32s - loss: 0.3593 - val_loss: 0.6526
Epoch 7/150
 - 32s - loss: 0.3535 - val_loss: 0.6559
Epoch 8/150
 - 32s - loss: 0.3500 - val_loss: 0.6246
Epoch 9/150
 - 32s - loss: 0.3478 - val_loss: 0.8568
Epoch 10/150
 - 32s - loss: 0.3468 - val_loss: 0.5786
Epoch 11/150
 - 32s - loss: 0.3466 - val_loss: 0.7132
Epoch 12/150
 - 32s - loss: 0.3398 - val_loss: 0.7247
Epoch 13/150
 - 32s - loss: 0.3395 - val_loss: 0.7392
Epoch 14/150
 - 32s - loss: 0.3372 - val_loss: 0.8823
Epoch 15/150
 - 32s - loss: 0.3371 - val_loss: 0.8267
Epoch 16/150
 - 32s - loss: 0.3350 - val_loss: 0.7443
Epoch 17/150
 - 32s - loss: 0.3299 - val_loss: 0.7005
Epoch 18/150
 - 31s - loss: 0.3309 - val_loss: 0.7277
Epoch 19/150
 - 31s - loss: 0.3268 - val_loss: 0.6005
Epoch 20/150
 - 32s - loss: 0.3271 - val_loss: 0.6335
Epoch 21/150
 - 32s - loss: 0.3234 - val_loss: 0.6545
Epoch 22/150
 - 32s - loss: 0.3215 - val_loss: 0.6686
Epoch 23/150
 - 32s - loss: 0.3215 - val_loss: 0.6301
Epoch 24/150
 - 32s - loss: 0.3203 - val_loss: 0.6709
Epoch 25/150
 - 32s - loss: 0.3184 - val_loss: 0.7112
Epoch 26/150
 - 32s - loss: 0.3152 - val_loss: 0.6461
Epoch 27/150
 - 32s - loss: 0.3196 - val_loss: 0.8408
Epoch 28/150
 - 32s - loss: 0.3151 - val_loss: 0.7127
Epoch 29/150
 - 32s - loss: 0.3112 - val_loss: 0.8083
Epoch 30/150
 - 32s - loss: 0.3114 - val_loss: 0.7214
Epoch 31/150
 - 32s - loss: 0.3086 - val_loss: 0.6954
Epoch 32/150
 - 32s - loss: 0.3084 - val_loss: 0.9393
Epoch 33/150
 - 32s - loss: 0.3067 - val_loss: 0.9304
Epoch 34/150
 - 31s - loss: 0.3043 - val_loss: 0.6946
Epoch 35/150
 - 31s - loss: 0.3066 - val_loss: 0.7145
Epoch 36/150
 - 32s - loss: 0.3033 - val_loss: 0.8369
Epoch 37/150
 - 32s - loss: 0.3035 - val_loss: 0.7509
Epoch 38/150
 - 32s - loss: 0.3024 - val_loss: 0.7238
Epoch 39/150
 - 32s - loss: 0.3027 - val_loss: 0.7886
Epoch 40/150
 - 32s - loss: 0.2984 - val_loss: 0.6829
Epoch 41/150
 - 32s - loss: 0.2956 - val_loss: 0.8065
Epoch 42/150
 - 32s - loss: 0.2930 - val_loss: 0.7021
Epoch 43/150
 - 32s - loss: 0.2950 - val_loss: 0.7029
Epoch 44/150
 - 32s - loss: 0.2950 - val_loss: 0.6914
Epoch 45/150
 - 32s - loss: 0.2927 - val_loss: 0.6478
Epoch 46/150
 - 32s - loss: 0.2892 - val_loss: 0.7752
Epoch 47/150
 - 32s - loss: 0.2899 - val_loss: 0.6901
Epoch 48/150
 - 32s - loss: 0.2846 - val_loss: 0.8951
Epoch 49/150
 - 32s - loss: 0.2839 - val_loss: 0.6550
Epoch 50/150
 - 31s - loss: 0.2838 - val_loss: 1.0535
Epoch 51/150
 - 31s - loss: 0.2842 - val_loss: 0.8013
Epoch 52/150
 - 32s - loss: 0.2811 - val_loss: 0.7669
Epoch 53/150
 - 32s - loss: 0.2823 - val_loss: 0.7817
Epoch 54/150
 - 32s - loss: 0.2789 - val_loss: 0.7965
Epoch 55/150
 - 32s - loss: 0.2792 - val_loss: 0.7219
Epoch 56/150
 - 32s - loss: 0.2788 - val_loss: 0.6977
Epoch 57/150
 - 32s - loss: 0.2749 - val_loss: 0.7622
Epoch 58/150
 - 32s - loss: 0.2737 - val_loss: 0.7887
Epoch 59/150
 - 32s - loss: 0.2751 - val_loss: 0.7425
Epoch 60/150
 - 32s - loss: 0.2713 - val_loss: 0.8789
Epoch 61/150
 - 32s - loss: 0.2665 - val_loss: 0.7339
Epoch 62/150
 - 32s - loss: 0.2732 - val_loss: 0.6962
Epoch 63/150
 - 32s - loss: 0.2676 - val_loss: 0.6992
Epoch 64/150
 - 32s - loss: 0.2698 - val_loss: 0.7641
Epoch 65/150
 - 32s - loss: 0.2572 - val_loss: 0.8551
Epoch 66/150
 - 31s - loss: 0.2579 - val_loss: 0.6945
Epoch 67/150
 - 32s - loss: 0.2523 - val_loss: 0.8185
Epoch 68/150
 - 32s - loss: 0.2505 - val_loss: 1.1193
Epoch 69/150
 - 32s - loss: 0.2483 - val_loss: 0.7901
Epoch 70/150
 - 32s - loss: 0.2415 - val_loss: 0.9292
Epoch 71/150
 - 32s - loss: 0.2409 - val_loss: 0.8507
Epoch 72/150
 - 32s - loss: 0.2346 - val_loss: 0.8530
Epoch 73/150
 - 32s - loss: 0.2242 - val_loss: 0.9175
Epoch 74/150
 - 32s - loss: 0.2238 - val_loss: 0.8605
Epoch 75/150
 - 32s - loss: 0.2234 - val_loss: 0.9165
Epoch 76/150
 - 32s - loss: 0.2196 - val_loss: 0.8074
Epoch 77/150
 - 32s - loss: 0.2077 - val_loss: 0.8976
Epoch 78/150
 - 32s - loss: 0.2065 - val_loss: 0.7719
Epoch 79/150
 - 32s - loss: 0.2074 - val_loss: 0.8720
Epoch 80/150
 - 32s - loss: 0.2014 - val_loss: 0.8282
Epoch 81/150
 - 32s - loss: 0.1958 - val_loss: 0.9026
Epoch 82/150
 - 31s - loss: 0.1876 - val_loss: 1.0184
Epoch 83/150
 - 32s - loss: 0.1869 - val_loss: 0.9034
Epoch 84/150
 - 32s - loss: 0.1783 - val_loss: 0.8874
Epoch 85/150
 - 32s - loss: 0.1714 - val_loss: 1.0970
Epoch 86/150
 - 32s - loss: 0.1661 - val_loss: 1.0236
Epoch 87/150
 - 32s - loss: 0.1673 - val_loss: 0.9438
Epoch 88/150
 - 32s - loss: 0.1739 - val_loss: 0.9468
Epoch 89/150
 - 32s - loss: 0.1645 - val_loss: 1.0644
Epoch 90/150
 - 32s - loss: 0.1594 - val_loss: 1.1579
Epoch 91/150
 - 32s - loss: 0.1532 - val_loss: 1.0603
Epoch 92/150
 - 32s - loss: 0.1486 - val_loss: 1.0848
Epoch 93/150
 - 32s - loss: 0.1447 - val_loss: 0.9887
Epoch 94/150
 - 32s - loss: 0.1453 - val_loss: 1.1153
Epoch 95/150
 - 32s - loss: 0.1355 - val_loss: 1.1859
Epoch 96/150
 - 32s - loss: 0.1420 - val_loss: 0.9625
Epoch 97/150
 - 31s - loss: 0.1334 - val_loss: 1.0990
Epoch 98/150
 - 31s - loss: 0.1367 - val_loss: 1.0302
Epoch 99/150
 - 32s - loss: 0.1292 - val_loss: 1.1515
Epoch 100/150
 - 32s - loss: 0.1245 - val_loss: 1.2657
Epoch 101/150
 - 32s - loss: 0.1255 - val_loss: 0.9146
Epoch 102/150
 - 32s - loss: 0.1238 - val_loss: 1.0924
Epoch 103/150
 - 32s - loss: 0.1195 - val_loss: 1.0683
Epoch 104/150
 - 32s - loss: 0.1189 - val_loss: 1.0903
Epoch 105/150
 - 32s - loss: 0.1157 - val_loss: 1.2398
Epoch 106/150
 - 32s - loss: 0.1147 - val_loss: 1.1906
Epoch 107/150
 - 32s - loss: 0.1193 - val_loss: 1.3132
Epoch 108/150
 - 32s - loss: 0.1084 - val_loss: 1.2463
Epoch 109/150
 - 32s - loss: 0.1196 - val_loss: 1.2034
Epoch 110/150
 - 32s - loss: 0.1075 - val_loss: 1.1820
Epoch 111/150
 - 32s - loss: 0.1003 - val_loss: 1.1484
Epoch 112/150
 - 32s - loss: 0.1012 - val_loss: 1.2567
Epoch 113/150
 - 31s - loss: 0.0948 - val_loss: 1.2248
Epoch 114/150
 - 31s - loss: 0.1008 - val_loss: 1.2283
Epoch 115/150
 - 32s - loss: 0.0988 - val_loss: 1.3955
Epoch 116/150
 - 32s - loss: 0.1008 - val_loss: 0.9790
Epoch 117/150
 - 32s - loss: 0.1045 - val_loss: 1.1757
Epoch 118/150
 - 32s - loss: 0.0910 - val_loss: 1.2919
Epoch 119/150
 - 32s - loss: 0.0960 - val_loss: 1.0587
Epoch 120/150
 - 32s - loss: 0.0920 - val_loss: 1.1515
Epoch 121/150
 - 32s - loss: 0.0886 - val_loss: 1.0610
Epoch 122/150
 - 32s - loss: 0.0870 - val_loss: 1.3439
Epoch 123/150
 - 32s - loss: 0.0961 - val_loss: 1.0379
Epoch 124/150
 - 32s - loss: 0.0933 - val_loss: 1.2053
Epoch 125/150
 - 32s - loss: 0.0856 - val_loss: 1.2625
Epoch 126/150
 - 32s - loss: 0.0801 - val_loss: 1.3693
Epoch 127/150
 - 32s - loss: 0.0771 - val_loss: 1.3175
Epoch 128/150
 - 32s - loss: 0.0779 - val_loss: 1.2490
Epoch 129/150
 - 31s - loss: 0.0737 - val_loss: 1.3525
Epoch 130/150
 - 30s - loss: 0.0733 - val_loss: 1.3461
Epoch 131/150
 - 30s - loss: 0.0745 - val_loss: 1.4103
Epoch 132/150
 - 30s - loss: 0.0768 - val_loss: 1.0497
Epoch 133/150
 - 32s - loss: 0.0756 - val_loss: 1.3241
Epoch 134/150
 - 32s - loss: 0.0712 - val_loss: 1.4161
Epoch 135/150
 - 32s - loss: 0.0766 - val_loss: 0.6489
Epoch 136/150
 - 32s - loss: 0.1100 - val_loss: 1.1013
Epoch 137/150
 - 32s - loss: 0.0768 - val_loss: 1.4464
Epoch 138/150
 - 32s - loss: 0.0694 - val_loss: 1.3852
Epoch 139/150
 - 32s - loss: 0.0634 - val_loss: 1.5059
Epoch 140/150
 - 32s - loss: 0.0609 - val_loss: 1.4373
Epoch 141/150
 - 32s - loss: 0.0629 - val_loss: 1.4345
Epoch 142/150
 - 32s - loss: 0.0651 - val_loss: 1.3593
Epoch 143/150
 - 32s - loss: 0.0671 - val_loss: 1.4540
Epoch 144/150
 - 32s - loss: 0.0676 - val_loss: 1.3393
Epoch 145/150
 - 32s - loss: 0.0636 - val_loss: 1.3541
Epoch 146/150
 - 32s - loss: 0.0677 - val_loss: 1.2685
Epoch 147/150
 - 31s - loss: 0.0629 - val_loss: 1.3789
Epoch 148/150
 - 30s - loss: 0.0578 - val_loss: 1.3858
Epoch 149/150
 - 32s - loss: 0.0579 - val_loss: 1.5230
Epoch 150/150
 - 32s - loss: 0.0532 - val_loss: 1.3502
Using TensorFlow backend.
